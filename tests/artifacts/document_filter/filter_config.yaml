
template_vars:
  do_cleanup: &do_cleanup False

  num_processes: &num_processes 64
  context_len: &context_len 4096

  document_filter:
    filter_source: &filter_source ./tests/artifacts/document_filter/inputs/*/*.json.gz
    filter_destination: &filter_destination ./tests/artifacts/document_filter/outputs
    exclude_tasks: &exclude_tasks
      - ".metadata._task_name == \"bool_q:1.0.0\""
      - ".metadata._task_name == \"cot_gsm8k\""
      - ".metadata._task_name == \"cot_gsm8k_ii\""
      - ".metadata._task_name == \"drop:2.0.0\""
      - ".metadata._task_name == \"lambada:1.0.0\""
      - ".metadata._task_name == \"math_dataset/algebra__linear_1d:1.0.0\""
      - ".metadata._task_name == \"natural_questions_open:1.0.0\""
      - ".metadata._task_name == \"race_high_is_this_the_right_answer\""
      - ".metadata._task_name == \"race_high_read_the_article_and_answer_the_question_no_option_\""
      - ".metadata._task_name == \"race_high_select_the_best_answer\""
      - ".metadata._task_name == \"race_high_select_the_best_answer_generate_span_\""
      - ".metadata._task_name == \"race_high_select_the_best_answer_no_instructions_\""
      - ".metadata._task_name == \"race_high_taking_a_test\""
      - ".metadata._task_name == \"race_high_write_a_multi_choice_question_for_the_following_article\""
      - ".metadata._task_name == \"race_high_write_a_multi_choice_question_options_given_\""
      - ".metadata._task_name == \"race_middle_is_this_the_right_answer\""
      - ".metadata._task_name == \"race_middle_read_the_article_and_answer_the_question_no_option_\""
      - ".metadata._task_name == \"race_middle_select_the_best_answer\""
      - ".metadata._task_name == \"race_middle_select_the_best_answer_generate_span_\""
      - ".metadata._task_name == \"race_middle_select_the_best_answer_no_instructions_\""
      - ".metadata._task_name == \"race_middle_taking_a_test\""
      - ".metadata._task_name == \"race_middle_write_a_multi_choice_question_for_the_following_article\""
      - ".metadata._task_name == \"race_middle_write_a_multi_choice_question_options_given_\""
      - ".metadata._task_name == \"task026_drop_question_generation\""
      - ".metadata._task_name == \"task027_drop_answer_type_generation\""
      - ".metadata._task_name == \"task028_drop_answer_generation\""
      - ".metadata._task_name == \"task073_commonsenseqa_answer_generation\""
      - ".metadata._task_name == \"task133_winowhy_reason_plausibility_detection\""
      - ".metadata._task_name == \"task134_winowhy_reason_generation\""
      - ".metadata._task_name == \"task135_winowhy_wrong_reason_generation\""
      - ".metadata._task_name == \"task136_winowhy_knowledge_categorization\""
      - ".metadata._task_name == \"task1564_triviaqa_answer_generation\""
      - ".metadata._task_name == \"task1565_triviaqa_classification\""
      - ".metadata._task_name == \"task1664_winobias_text_generation\""
      - ".metadata._task_name == \"task309_race_answer_generation\""
      - ".metadata._task_name == \"task310_race_classification\""
      - ".metadata._task_name == \"task311_race_question_generation\""
      - ".metadata._task_name == \"task340_winomt_classification_gender_pro\""
      - ".metadata._task_name == \"task341_winomt_classification_gender_anti\""
      - ".metadata._task_name == \"task342_winomt_classification_profession_pro\""
      - ".metadata._task_name == \"task343_winomt_classification_profession_anti\""
      - ".metadata._task_name == \"task350_winomt_classification_gender_identifiability_pro\""
      - ".metadata._task_name == \"task351_winomt_classification_gender_identifiability_anti\""
      - ".metadata._task_name == \"task453_swag_answer_generation\""
      - ".metadata._task_name == \"task454_swag_incorrect_answer_generation\""
      - ".metadata._task_name == \"task455_swag_context_generation\""
      - ".metadata._task_name == \"task649_race_blank_question_generation\""
      - ".metadata._task_name == \"task834_mathdataset_classification\""
      - ".metadata._task_name == \"task835_mathdataset_answer_generation\""
      - ".metadata._task_name == \"trivia_qa/rc:1.1.0\""


pipeline:
  # remove unwanted tasks from the dataset
  - id: document_filter
    args:
      num_processes: *num_processes
      documents_glob: *filter_source
      shard_from_source_regex: (shard-\d+-of-\d+)
      # below here are the args for the "mix" part
      streams:
        - name: filtered
          documents: null
          attributes: [ ]
          output:
            path: *filter_destination
            max_size_in_bytes: 500000000
          filter:
            syntax: jq
            exclude: *exclude_tasks
    call_kwargs:
      cleanup: *do_cleanup
