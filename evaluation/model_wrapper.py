from typing import List, Tuple
import pdb
import sys

import torch
import torch.nn.functional as F

from tqdm import tqdm
from transformers import StoppingCriteriaList, MaxLengthCriteria
from accelerate import DistributedType, find_executable_batch_size

sys.path.append("./lm-evaluation-harness")
from lm_eval.api.model import LM
from lm_eval.models.utils import pad_and_concat, Collator, MultiTokenEOSCriteria, clear_torch_cache
from lm_eval.utils import get_rolling_token_windows, make_disjoint_window, eval_logger

from utils import Registry
from model.kv_cache import MultiLayerCache
from inference import *


class EvalHarnessModelWrapper(LM):

    def __init__(self, model, tokenizer, accelerator, batch_size, device="cuda", max_gen_toks=256, max_batch_size=64):
        super().__init__()
        self._model = model
        self._model.eval()
        self.tokenizer = tokenizer
        self.device = device
        self.batch_size = batch_size
        self.max_gen_toks = max_gen_toks
        self.max_batch_size = max_batch_size
        self.auto_batch_size = None

        if accelerator.num_processes > 1:
            # make DDP or FSDP model
            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU]
            if accelerator.distributed_type == DistributedType.FSDP:
                self._model = accelerator.prepare(self._model)
            else:
                self._model = accelerator.prepare_model(self._model, evaluation_mode=True)
            self.accelerator = accelerator
            self._rank = self.accelerator.local_process_index
            self._world_size = self.accelerator.num_processes
            device = f"{accelerator.device}"

        self._device = torch.device(device)

    @property
    def model(self):
        # returns the model, unwrapping it if using Accelerate
        if hasattr(self, "accelerator"):
            return self.accelerator.unwrap_model(self._model)
        else:
            return self._model

    @property
    def prefix_token_id(self):
        # it is used as prefix for loglikelihood
        if self.tokenizer.bos_token_id is not None:
            return self.tokenizer.bos_token_id
        return self.tokenizer.eos_token_id

    def _encode_pair(self, context, continuation):
        n_spaces = len(context) - len(context.rstrip())
        if n_spaces > 0:
            continuation = context[-n_spaces:] + continuation
            context = context[:-n_spaces]
        whole_enc = self.tokenizer.encode(context + continuation)
        context_enc = self.tokenizer.encode(context)
        context_enc_len = len(context_enc)
        continuation_enc = whole_enc[context_enc_len:]
        return context_enc, continuation_enc

    def loglikelihood(self, requests, disable_tqdm = False) -> List[Tuple[float, bool]]:
        """Compute log-likelihood of generating a continuation from a context.
        Downstream tasks should attempt to use loglikelihood instead of other
        LM calls whenever possible.

        :param requests: list[Instance]
            A list of Instance objects, with property `args` which returns a tuple (context, continuation).
            `context: str`
                Context string. Implementations of LM must be able to handle an
                empty context string.
            `continuation: str`
                The continuation over which log likelihood will be calculated. If
                there is a word boundary, the space should be in the continuation.
                For example, context="hello" continuation=" world" is correct.

        :return: list[tuple[float, bool]]
            A list of pairs (logprob, isgreedy)
            `logprob: float`
                The log probability of `continuation`.
            `isgreedy`:
                Whether `continuation` would be generated by greedy sampling from `context`.
        """
        inputs = []
        targets = []
        lengths = []
        pbar = tqdm(total=len(requests), desc="generating loglikelihood", disable=disable_tqdm)

        for req in requests:
            # tokenize context and continuation
            ctx, cont = req.args
            if ctx == "":
                ctx_ids = [self.prefix_token_id]
                cont_ids = self.tokenizer.encode(cont)
            else:
                ctx_ids, cont_ids = self._encode_pair(ctx, cont)
            # ensure the continuation fits in the model context window
            ctx_len = len(ctx_ids)
            cont_len = len(cont_ids)
            if cont_len > self._model.max_len:
                raise ValueError("Continuation too long to fit in max_len")
            # concat context & continuation, truncate to max len; don't include cont_ids[-1] b/c we predict it
            input_ids = torch.tensor(ctx_ids + cont_ids[:-1], dtype=torch.long)[-self._model.max_len:]
            target_ids = torch.tensor(cont_ids, dtype=torch.long)
            # adjust ctx_len if we truncated the input
            if ctx_len + cont_len > self._model.max_len:
                ctx_len = self._model.max_len - cont_len
                eval_logger.warning("Context too long, truncating")
            inputs.append(input_ids)
            targets.append(target_ids)
            lengths.append((ctx_len, cont_len))
            pbar.update(1)

        pbar.close()
        return self._get_likelihood_of_continuation_given_context(inputs, targets, lengths)

    def loglikelihood_rolling(self, requests) -> List[Tuple[float, bool]]:
        """
        Returns a list, likelihoods where likelihoods[i] contains the log likelihood of requests[i]
        """
        out_list = []
        for req in requests:
            inputs = []
            targets = []
            lengths = []
            text, = req.args
            token_ids = self.tokenizer.encode(text)
            # get window w of [ctx, cnt] where |w| = max_len + 1. Note |ctx| > 1 if |cnt| is < max_len
            windows = get_rolling_token_windows(
                token_list=token_ids,
                prefix_token=self.prefix_token_id,
                max_seq_len=self._model.max_len,
                context_len=1,
            )
            ctx_cont_pairs = [make_disjoint_window(w) for w in windows]
            for ctx_ids, cont_ids in ctx_cont_pairs:
                # exclude cont[-1] from the input b/c we predict it at the last time step
                input_ids = torch.tensor(ctx_ids+cont_ids[:-1], dtype=torch.long)
                target_ids = torch.tensor(cont_ids, dtype=torch.long)
                inputs.append(input_ids)
                targets.append(target_ids)
                lengths.append((len(ctx_ids), len(cont_ids)))
            # TODO: we can batch this more if it's slow
            log_probs, _ = zip(*self._get_likelihood_of_continuation_given_context(inputs, targets, lengths))
            out_list.append(sum(log_probs))
        return out_list

    @torch.inference_mode()
    def _detect_batch_size_using_input_at_pos(
        self,
        i,
        iterable,
        get_max_len=lambda xs: len(xs[0]),
        pos=0,
        do_generate=False,
        n_forward=5
    ):
        """
        Detects batch size when i == pos using the max length of the input at position i
        get_max_len_at_pos: takes in the iterator to batch and the position i, and returns max length of the input at position i
        """
        if i == pos:
            max_len = get_max_len(iterable[i])
            self.auto_batch_size = self._detect_batch_size(max_len, n_forward, do_generate)
        else:
            return self.auto_batch_size

    @torch.inference_mode()
    def _detect_batch_size(self, max_length, n_forward, do_generate):
        """
        Adapted from https://github.com/EleutherAI/lm-evaluation-harness/blob/42dc244867889a19ae80847254a481f446f6e4b7/lm_eval/models/huggingface.py#L660

        max_length: int, the maximum number of tokens in any model input
        """
        max_length = min(max_length, self._model.max_len)

        @find_executable_batch_size(starting_batch_size=self.max_batch_size)
        def forward_decode(batch_size):
            test_batch = [[1]*(max_length - n_forward) for _ in range(batch_size)]
            stop_criteria_list = StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])
            decoder = GreedyDecoding()
            _ = decoder(
                prompt_list=test_batch,
                model=self._model,
                stop_criteria=stop_criteria_list,
                temperature=1.0,
                pad_token_id=self.tokenizer.pad_token_id,
                max_length=max_length,
                device=self.device,
            )
            return batch_size

        @find_executable_batch_size(starting_batch_size=self.max_batch_size)
        def forward_batch(batch_size):
            # we clear cache b/c this function is auto called with reduced batch size
            # and we don't want a diff prev batch size for cache to cause error
            test_batch = torch.ones((batch_size, max_length), device=self.device).long()
            for i in range(n_forward):
                _ = F.log_softmax(self._model(test_batch), dim=-1)
            del test_batch
            return batch_size

        if do_generate:
            batch_size = forward_decode()
        else:
            batch_size = forward_batch()

        if self._world_size > 1:
            # if multi-GPU, always take minimum over all selected batch sizes
            max_rnk_bs = torch.tensor([batch_size], device=self.device)
            batch_size = min(self.accelerator.gather(max_rnk_bs).cpu().detach().numpy().tolist())
            del max_rnk_bs

        eval_logger.info(f"detected auto batch size of: {batch_size}")
        clear_torch_cache()
        return batch_size

    @torch.inference_mode()
    def _get_likelihood_of_continuation_given_context(self, inputs, targets, lengths):
        """
        Returns a list of tuples L[(log_prob, is_greedy_match)], where L[i] contains the log probability
        of the continuation given the context in inputs[i], and is_greedy_match is a boolean indicating
        whether the continuation would be generated by greedy sampling from the context.

        Inputs: List[torch.Tensor], where the tensor contains the concatenated context and continuation[:-1]
        Targets: List[torch.Tensor], where the tensor contains the full continuation
        Lengths: List[Tuple], where each tuple contains (context_len, continuation_len)
        """
        out_list = []
        sorted_reqs = Collator(list(zip(inputs, targets, lengths)), sort_fn=lambda x: -len(x[0]), group_by=None)

        batch_fn = self._detect_batch_size_using_input_at_pos if self.batch_size == "auto" else None
        batch_list = sorted_reqs.get_batched(n=self.batch_size, batch_fn=batch_fn)

        for batch in batch_list:
            input_batch, target_batch, length_batch = [list(b) for b in zip(*batch)]
            max_length = max([len(x) for x in input_batch])
            input_batch = pad_and_concat(max_length=max_length, tensors=input_batch, padding_side="right")
            input_batch = input_batch.to(self.device)

            logits = self._model(input_batch)
            # (N, L_max, V)
            log_probs = F.log_softmax(logits, dim=-1)

            for p, input_ids, cont_ids, (ctx_len, cont_len) in zip(log_probs, input_batch, target_batch, length_batch):
                cont_ids = cont_ids.to(self.device)
                # (|ctx|, V)
                cont_prob = p[ctx_len-1:ctx_len+cont_len-1]
                # (|ctx|)
                greedy_ids = torch.argmax(cont_prob, dim=-1)
                is_greedy = (greedy_ids == cont_ids).all()
                # index into cont_prob as if it were a 1d array
                cont_idxs = cont_ids + torch.arange(cont_prob.shape[0]).to(self.device) * cont_prob.shape[1]
                target_prob = torch.take(cont_prob, cont_idxs).sum()
                out_list.append((target_prob.item(), is_greedy.item()))

        out_list = sorted_reqs.get_original(out_list)
        return out_list

    def generate_until(self, requests, disable_tqdm: bool = False) -> List[str]:
        """Generate greedily until a stopping sequence

        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context, until).
            context: str
                Context string
            until: [str]
                The string sequences to generate until. These string sequences
                may each span across multiple tokens, or may be part of one token.
        :return: list[str]
            A list of strings continuation
            continuation: str
                The generated continuation.
        """
        cont_list = []
        pbar = tqdm(total=len(requests), desc="generating cont given ctx", disable=disable_tqdm)

        # TODO: can remove if this is too slow
        args_list = []
        max_len_list = []
        for req in requests:
            args_list.append(req.args)
            ctx_len = len(self.tokenizer.encode(req.args[0]))
            max_gen_toks = req.args[1].get("max_gen_toks", self.max_gen_toks)
            max_len = min(self._model.max_len, ctx_len + max_gen_toks, req.args[1].get("max_length", float("inf")))
            max_len_list.append(max_len)

        # group requests by kwargs and sort from longest to shortest
        sorted_reqs = Collator(
            args_list,
            sort_fn=lambda x: (-len(self.tokenizer.encode(x[0])), x[0]),
            group_by="gen_kwargs",
            group_fn=lambda x: x[1],
        )

        batch_size = self.batch_size
        if self.batch_size == "auto":
            batch_size = self._detect_batch_size(max(max_len_list), n_forward=5, do_generate=True)

        #batch_size = self._detect_batch_size(self._model.max_len-5, n_forward=5, disable_cache=False)
        batch_list = sorted_reqs.get_batched(n=batch_size)

        for batch in batch_list:
            ctx_list, all_gen_kwargs = [list(a) for a in zip(*batch)]
            # we grouped by gen kwargs so the gen kwargs for this batch are all the same
            gen_kwargs = all_gen_kwargs[0]

            # get stop sequence
            until = gen_kwargs.pop("until", None)
            if isinstance(until, str):
                until = [until]
            elif not isinstance(until, list):
                raise ValueError(f"Expected `kwargs['until']` to be of type Union[str,list] but got {until}")

            # add EoS to stop sequence
            if not until:
                until = [self.tokenizer.eos_token]
            else:
                until.append(self.tokenizer.eos_token)

            max_gen_toks = gen_kwargs.pop("max_gen_toks", self.max_gen_toks)
            max_ctx_len = self._model.max_len - max_gen_toks

            ctx_ids = [ids[-max_ctx_len:] for ids in self.tokenizer(ctx_list)["input_ids"]]
            ctx_lens = [len(ids) for ids in ctx_ids]

            if "max_length" not in gen_kwargs:
                gen_kwargs["max_length"] = max(ctx_lens) + max_gen_toks
            gen_kwargs["max_length"] = min(gen_kwargs["max_length"], self._model.max_len)

            out = self._model_generate(context=ctx_ids, stop=until, **gen_kwargs)
            assert len(out) == len(ctx_ids)

            for ctx_str, ctx_len, out_ids in zip(ctx_list, ctx_lens, out):
                cont = self.tokenizer.decode(out_ids[ctx_len:])
                # remove use secondary stop seqs that we want to cut off
                # approach copied from eval harness huggingface.py
                for term in until:
                    if len(term) > 0:
                        cont = cont.split(term)[0]
                cont_list.append(cont)
                self.cache_hook.add_partial("generate_until", (ctx_str, gen_kwargs), cont)
                pbar.update(1)

        cont_list = sorted_reqs.get_original(cont_list)
        pbar.close()
        return cont_list


    def _model_generate(self, context, max_length, stop, do_sample=False, temperature=0.0):
        decoding_type = None
        if do_sample and temperature > 0:
            decoding_type = "multinomial_decoding"
        elif do_sample is False or temperature == 0:
            decoding_type = "greedy_decoding"
        else:
            raise ValueError(f"combination of do_sample: {do_sample} and temperature: {temperature} not supported")

        decoder_cls = Registry.get(decoding_type)
        decoder = decoder_cls()

        stop_criteria_list = [MaxLengthCriteria(max_length=max_length)]
        eos_lookback_len = max([len(ctx) for ctx in context])
        stop_criteria_list += [
            MultiTokenEOSCriteria(stop_seq, self.tokenizer, eos_lookback_len, batch_size=len(context))
            for stop_seq in stop
        ]
        stop_criteria = StoppingCriteriaList(stop_criteria_list)

        out_padded, _, out_lens = decoder(
            prompt_list=context,
            model=self._model,
            stop_criteria=stop_criteria,
            temperature=temperature,
            pad_token_id=self.tokenizer.pad_token_id,
            max_length=max_length,
            device=self.device,
        )

        out = [out_padded[i, :l].tolist() for i, l in enumerate(out_lens)]
        return out

