template_vars:
  num_processes: &num_processes 64
  context_len: &context_len 4096

  downloader:
    download_destination: &download_destination /local_out/download-$dataset/documents

  elastic_search_base_args: &es_args
    index_name: &index_name $dataset
    hosts:
      - http://elasticsearch:9200
    request_timeout: 600
    max_retries: 10
    pbar_interval: 100_000
    num_processes: *num_processes

  elastic_search_indexer:
    source_glob: &indexer_source_glob /local_out/download-$dataset/documents/*/*.$source_extension
    chunk_size: &chunk_size_to_index 500 
    property_to_jsonpath: &property_to_jsonpath
      id: url
      text: text
      created: metadata.WARC-Date
      source: source

  contaminated_span_tagger:
    ckpt_interval: &ckpt_interval 1_000_000

  decontaminator:
    decontam_metadata: &decontam_metadata /local_out/decontam-$dataset-metadata
 
  map_jsonl:
    map_source: &map_source /local_out/download-$dataset/documents/*/*.$source_extension
    map_destination: &map_destination /local_out/map-$dataset
    map_metadata: &map_metadata /local_out/map-$dataset-metadata

  split_maker:
    split_source: &split_source /local_out/map-$dataset/*/*.$source_extension
    split_destination: &split_destination /local_out/split-$dataset

  split_writer:
    tokenizer: &tokenizer
      id: tiktoken
      args: {}
    property_to_default: &property_to_default
      source: $dataset
    # at 4096 seq length this is ~2.4k samples per chunk
    num_tokens_per_chunk: &num_tokens_per_chunk 10_000_000
    mds_size_limit: &mds_size_limit 130mb
    # train
    train_source: &train_source /local_out/split-$dataset/*/train/*.$source_extension
    train_destination: &train_destination s3://$dataset-mds-decontam-ctx-4096-tokenizer-tiktoken/train
    # dev
    dev_source: &dev_source /local_out/split-$dataset/*/dev/*.$source_extension
    dev_destination: &dev_destination s3://$dataset-mds-decontam-ctx-4096-tokenizer-tiktoken/dev


pipeline:
  # download the data to decontaminate
  - id: downloader
    args:
      urls_path: *urls_path
      destination_prefix: *download_destination
      num_shards: *num_shards
      num_processes: *num_processes
      cleanup: *do_cleanup
      is_distributed: *is_distributed
      src_location: *source_location
    call_kwargs: {}
  
  ## index the data to decontaminate
  - id: elastic_search_indexer
    args:
      <<: *es_args
      num_documents_to_index: *num_documents_to_index
      source_glob: *indexer_source_glob
      chunk_size: *chunk_size_to_index
      metadata_prefix: *decontam_metadata
      property_to_jsonpath:
        <<: *property_to_jsonpath
      properties:
        created:
          type: date
          format: strict_date_optional_time||epoch_millis
        id:
          type: text
          analyzer: standard
        text:
          type: text
          analyzer: standard
        path:
          type: text
          analyzer: standard
        line_num:
          type: integer

  