wandb:
  project_name: scaling_eval
  run_name: null

compute:
  device: cuda
  is_distributed: True

seed: &seed 414

eval_vars:
  do_apply_chat_template: &do_apply_chat_template False
  ifeval_max_gen_toks: &ifeval_max_gen_toks 1792


include: configs/eval/scaling_eval/_scaling_law_eval_tasks_yaml

# plan
# [X] 1. add remaining tasks
# [X] 2. review the models to use so we get a good sweep across PF-days 
# [ ] 3. run all the things
# [ ] 4. move over to SFT and DPO


baselines:
  ########################################################################################
  #                                     Base Models                                      #
  ########################################################################################

  # models removed

  #pythia-1-dot-4b-deduped-69b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-1.4b-deduped,revision=step33000

  #pythia-6-dot-9b-deduped-153b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-6.9b-deduped,revision=step73000

  #olmo_1b_toks_75b:
  #  id: hf
  #  args: pretrained=jasonkrone/olmo_1b_toks_75b,tokenizer=allenai/OLMo-1B-hf

  #olmo_7b_toks_168b:
  #  id: hf
  #  args: pretrained=jasonkrone/olmo_7b_toks_168b,tokenizer=allenai/OLMo-7B-hf

  #olmo_7b_toks_449b:
  #  id: hf
  #  args: pretrained=jasonkrone/olmo_7b_toks_449b,tokenizer=allenai/OLMo-7B-hf

  #olmo_7b_toks_600b:
  #  id: hf
  #  args: pretrained=jasonkrone/olmo_7b_toks_600b,tokenizer=allenai/OLMo-7B-hf


  ########### pythia 1.4B ##########
  #pythia-1-dot-4b-deduped-27b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-1.4b-deduped,revision=step13000
  #pythia-1-dot-4b-deduped-48b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-1.4b-deduped,revision=step23000
  #pythia-1-dot-4b-deduped-111b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-1.4b-deduped,revision=step53000

  ######### pythia 2.8B #########
  #pythia-2dot8b-deduped-27b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-2.8b-deduped,revision=step13000
  #pythia-2dot8b-deduped-48b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-2.8b-deduped,revision=step23000
  #pythia-2dot8b-deduped-111b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-2.8b-deduped,revision=step53000

  ############# pythia 6.9B ##########
  #pythia-6-dot-9b-deduped-27b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-6.9b-deduped,revision=step13000
  #pythia-6-dot-9b-deduped-48b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-6.9b-deduped,revision=step23000
  #pythia-6-dot-9b-deduped-111b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-6.9b-deduped,revision=step53000
  #pythia-6-dot-9b-deduped-300b-toks:
  #  id: hf
  #  args: pretrained=EleutherAI/pythia-6.9b-deduped

  ############ OLMo 1B ##########
  #olmo_1b_toks_21b:
  #  id: hf
  #  args: pretrained=jasonkrone/olmo_1b_toks_21b,tokenizer=allenai/OLMo-1B-hf

  #olmo_1b_toks_50b:
  #  id: hf
  #  args: pretrained=jasonkrone/olmo_1b_toks_50b,tokenizer=allenai/OLMo-1B-hf

  #olmo_1b_toks_126:
  #  id: hf
  #  args: pretrained=jasonkrone/olmo_1b_toks_126,tokenizer=allenai/OLMo-1B-hf

  ############# OLMo 7b ##########
  olmo_7b_toks_27b:
    id: hf
    args: pretrained=allenai/OLMo-7B,revision=step6000-tokens27B,dtype=bfloat16
  olmo_7b_toks_53b:
    id: hf
    args: pretrained=allenai/OLMo-7B,revision=step12000-tokens53B,dtype=bfloat16
  olmo_7b_toks_106b:
    id: hf
    args: pretrained=allenai/OLMo-7B,revision=step24000-tokens106B,dtype=bfloat16
  olmo_7b_toks_301b:
    id: hf
    args: pretrained=allenai/OLMo-7B,revision=step68000-tokens301B,dtype=bfloat16


# model info will be set by scripts to the baseline info above
model:
  id: null
  args: null

test_eval:
  clear_cache: True
  include_path: ./tasks
  log_samples: True
  limit: null
  output_path: /output/baselines
  tasks:
    <<: *scaling_law_eval_tasks
  