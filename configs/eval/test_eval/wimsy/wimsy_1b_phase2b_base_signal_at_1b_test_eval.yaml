seed: &seed 414

##########################################
#                 Imports                #
##########################################

model:
  use_compile_profiler: False
  do_offload_activations: False
  use_meta_device: True
  use_cache: False
  do_compile: True
  parallelism: fsdp_model
  id: decoder_lm
  initializer: null
  precision: bf16
  vocab_size: 100288
  n_layers: 16
  d_model: 2048
  n_heads: 32
  n_kv_heads: 1
  d_hidden: 8192
  use_bias: False
  use_qk_norm: True
  max_len: &context_len 4096
  transformer_block: res_post_norm_transformer_block
  attn: base_attn
  mlp: mlp_swiglu
  norm: rms_norm
  pos_encoding: identity
  qk_encoding: rotary_encoding
  rope_base: 10_000
  p_dropout: 0.0
  init_stdev: null

# didn't include b/c we had to change the attn from flash to base_attn cuz fp32
# configs/train/wimsy/wimsy_1b/architectures/_llama_1b_architecture_yaml
include: configs/eval/test_eval/_test_evals_with_signal_at_1b_yaml

wandb:
  project_name: wimsy_test_eval
  run_name: wimsy_1b_phase2b_base_50b_train_toks 

compute:
  device: cuda
  is_distributed: True
  backend: nccl

############################################
#              Checkpointing               #
############################################

ckpt:
  strict: False
  load_dataset_loader: False
  load_optimizer: False
  checkpointer: fsdp_checkpointer
  remote_output_dir: s3://jpt-output
  local_output_dir: /output
  copy_dir: /copy
  strip_prefix: _orig_mod.

############################################
#                 Eval                     #
############################################

test_eval:
  output_path: null 
  ckpt_path: /output/pretrain-phase2-model-wimsy1b-phase1b-data-phase2b-base/checkpoints/21798.pt
  tokenizer:
    id: tiktoken
    args: {}
  include_path: ./tasks
  log_samples: True
  limit: null
  tasks:
    <<: *signal_at_1b_eval_tasks