extra_vars:
  grad_accum_steps: &grad_accum_steps 2


##########################################
#                 Imports                #
##########################################

# mini run settings
include: configs/train/wimsy/wimsy_1b/architectures/_llama_1b_architecture_yaml
include: configs/train/wimsy/wimsy_1b/shared_hyper_params/_mini_run_shared_hyper_params_yaml
include: configs/train/wimsy/wimsy_1b/phase_hyper_params/_wimsy_1b_mini_run_hyper_params_yaml
include: configs/train/wimsy/data/_mini_run_data_mixes_yaml
include: configs/train/wimsy/_wimsy_template_yaml

##########################################
#             Weights & Biases           #
##########################################

wandb:
  project_name: wimsy
  run_name: 12-27-24-cudnn-attn-llama-1b-model-mini-run-data-phase-1-hyperparams

############################################
#              Checkpointing               #
############################################

ckpt:
  checkpointer: fsdp_checkpointer
  strip_prefix: _orig_mod.
  remote_output_dir: s3://jpt-output
  local_output_dir: /output
  copy_dir: /copy
  ckpt_interval: *ckpt_interval


###########################################
#                  Data                   #
###########################################

data:
  data_loader:
    id: streaming_data_loader
    args:
      pin_memory: True
      batch_size: *batch_size
      num_workers: *num_workers

  train_dataset:
    id: streaming_text_dataset
    args:
      load_mask: False
      streams:
        *pretrain_phase_1a_train_mix
      batch_size: *batch_size
      batching_method: random
      shuffle: True
      shuffle_seed: *seed
      keep_zip: True

  dev_dataset:
    id: streaming_text_dataset
    args:
      load_mask: False
      streams:
        *pretrain_phase_1a_dev_mix
      batch_size: *batch_size
      batching_method: random
      shuffle: True
      shuffle_seed: *seed
      keep_zip: True


