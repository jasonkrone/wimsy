name: train-7b-fsdp-2xh100

resources:
  #accelerators: {H100:8}
  instance_type: p5.48xlarge
  cloud: aws
  region: us-east-2
  #zone: us-east-2a
  image_id: ami-07bd98b6070307ae7 # ami-0ec6aaba9a8600809 # us 1 east Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.2.0 (Ubuntu 20.04) 20240420

# this is the distributed part
num_nodes: 1

file_mounts:
  /output:
    source: s3://jpt-output
    mode: MOUNT

workdir: ~/Developer/jpt

setup: |
  echo "setup"

run: |
  #requirements_path="requirements/requirements.txt"
  #env_name="train"
  #source ./bash/setup.sh; activate_or_create_conda_env $env_name $requirements_path
  #source ./bash/setup.sh; clean_tmp
  #
  #pip uninstall mosaicml-streaming -y
  #
  #sudo chmod go+rw /output
  #sudo mkdir /copy
  #sudo chmod go+rw /copy
  #
  #kill $(lsof -ti :8008 | head -n 1)
  #
  #num_nodes=`echo "$SKYPILOT_NODE_IPS" | wc -l`
  #master_addr=`echo "$SKYPILOT_NODE_IPS" | head -n1`
  #nproc_per_node=${SKYPILOT_NUM_GPUS_PER_NODE}
  #node_rank=${SKYPILOT_NODE_RANK}

  #echo "num_nodes: $num_nodes"
  #echo "master_addr: $master_addr"
  #echo "nproc_per_node: $nproc_per_node"
  #echo "node_rank: $node_rank"
  #
  #if [ $node_rank -eq 0 ]; then
  #  source run.sh ; run_7b_fsdp
  #fi
  
  # On AWS, the EFA and OFI paths enable NCCL to use optimized networking.
  #export LD_LIBRARY_PATH=/opt/nccl/build/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/aws-ofi-nccl/lib:/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda:/usr/local/cuda/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:$LD_LIBRARY_PATH
  #export FI_EFA_SET_CUDA_SYNC_MEMOPS=0
  #
  #source keys.env; TORCH_CUDNN_SDPA_ENABLED=1 torchrun  \
  #--nproc_per_node=$nproc_per_node \
  #--node_rank=$node_rank \
  #--nnodes=$num_nodes \
  #--master_addr=$master_addr \
  #--master_port=8008 \
  #-m main --config ./configs/train/7b_fsdp/train_7b_fsdp_neox.yaml

