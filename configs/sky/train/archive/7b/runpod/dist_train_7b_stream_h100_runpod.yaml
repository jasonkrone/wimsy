name: train-dist-7b

resources:
  accelerators: {H100:8}
  cloud: runpod

num_nodes: 1

file_mounts:
  /output:
    source: s3://jpt-output
    mode: MOUNT

workdir: ~/Developer/jpt

setup: |
  echo "setup"

run: |
  #export LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | sed 's|:/usr/local/cuda/lib64||g; s|/usr/local/cuda/lib64:||g; s|/usr/local/cuda/lib64||g')

  requirements_path="requirements/requirements.txt"
  env_name="train"
  source ./bash/setup.sh; activate_or_create_conda_env $env_name $requirements_path
  source ./bash/setup.sh; clean_tmp
  
  pip uninstall mosaicml-streaming -y
  
  sudo chmod go+rw /output
  sudo mkdir /copy
  sudo chmod go+rw /copy
  
  num_nodes=`echo "$SKYPILOT_NODE_IPS" | wc -l`
  master_addr=`echo "$SKYPILOT_NODE_IPS" | head -n1`
  nproc_per_node=${SKYPILOT_NUM_GPUS_PER_NODE}
  node_rank=${SKYPILOT_NODE_RANK}

  echo "num_nodes: $num_nodes"
  echo "master_addr: $master_addr"
  echo "nproc_per_node: $nproc_per_node"
  echo "node_rank: $node_rank"
  
  echo "========= running with flash attn ========"
  echo "$master_addr"

  source keys.env; torchrun  \
  --nproc_per_node=$nproc_per_node \
  --node_rank=$node_rank \
  --nnodes=$num_nodes \
  --master_addr=$master_addr \
  --master_port=8008 \
  -m training.train --config ./configs/train/train_7b_fsdp.yaml
